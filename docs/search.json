[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Portfolio",
    "section": "",
    "text": "This site is meant to document projects I’m working on and the ones I prefer never to return to. If you’ve reached this page through my twitter, feedback can be directed there."
  },
  {
    "objectID": "posts-hockey/index.html",
    "href": "posts-hockey/index.html",
    "title": "Hockey Analytics",
    "section": "",
    "text": "Welcome to my hockey analytics blog.\n\n\n\n\n\n\nDate\n\n\nTitle\n\n\n\n\n\n\nMay 29, 2025\n\n\nProspect Models\n\n\n\n\nSep 30, 2022\n\n\nCorrecting Player Positions\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts-hockey/prospect-models/index.html",
    "href": "posts-hockey/prospect-models/index.html",
    "title": "Prospect Models",
    "section": "",
    "text": "Some work on drafting prospects.\n\n\n\n\n\n\nDate\n\n\nTitle\n\n\n\n\n\n\nMay 30, 2025\n\n\nPart 1\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts-misc/index.html",
    "href": "posts-misc/index.html",
    "title": "Random Stuff",
    "section": "",
    "text": "Sometimes we stuble upon problems that lead to pleasant solutions. Mathematicians will describe them as elegant. I’ve avoided that descriptor due to my particular fondness for the jank and trivial. This section will contain a mixed bag of scraps from unfinished projects, possibly some solutions to interesting textbook problems."
  },
  {
    "objectID": "posts-misc/pipelines/index.html",
    "href": "posts-misc/pipelines/index.html",
    "title": "Partitioning a Pipeline",
    "section": "",
    "text": "Here’s a simple use case for applying connectivity constraints in Hierarchical Clustering. Our goal is to partition a raster image of the Canadian pipeline such that all pixels in a cluster will be connected through a sequence of adjacent members.\nSee here for the data source: https://ssc.ca/en/case-study/what-geographical-factors-are-associated-pipeline-incidents-involve-spills\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt \n\nfrom collections import deque\nfrom scipy.spatial.distance import cdist\nfrom sklearn.cluster import AgglomerativeClustering\nimg = plt.imread('gdm-Pipelines_EN.png')\nfig, ax = plt.subplots(figsize=(10,10))\nplt.imshow(img);\n\nLet’s ignore any information colour is providing here and convert to a binary image. We’ll also crop out any unnecessary whitespace.\n# convert to binary image\npixels = img[:,:,3] &gt; 0\n\n# crop out whitespace\nbot_crop = np.argmax(pixels.max(axis=1))\ntop_crop = pixels.shape[0] - np.argmax(pixels.max(axis=1)[::-1])\nleft_crop = np.argmax(pixels.max(axis=0))\nright_crop = pixels.shape[1] - np.argmax(pixels.max(axis=0)[::-1])\npixels = pixels[bot_crop:top_crop,left_crop:right_crop]\n\nplt.imshow(pixels,cmap='binary');\n\nNow let’s convert the raster to a graph where each node represents a coloured cell and each edge indicates two cells that are adjacent by a queen’s move one cell away.\nScikit-learn’s AgglomerativeClustering requires a connected graph so we’ll need to determine all the disconnected components using breadth-first search. Then we’ll use Kruskal’s algorithm to find the minimum spanning tree that connects all components and add those edges to the adjacency matrix.\n# convert nonempty cells to nodes\nnodes = np.array(np.where(pixels == 1)).T\nn_nodes = len(nodes)\n\n# create adjacency matrix by assigning edges to adjacent pixels\ndist_matrix = cdist(nodes,nodes)\nadj_matrix = 1*(dist_matrix&lt;2)\n\n# find all disconnected components\ncomponent_labels = np.zeros(n_nodes) - 1\ncomponentId = 0\nQ = deque()\nfor i in range(len(nodes)):\n    if component_labels[i] == -1:\n        component_labels[i] = componentId\n        Q.append(i)\n        while len(Q) &gt; 0:       \n            for j in np.where(adj_matrix[Q[0]] == 1)[0]:\n                if component_labels[j] == -1:\n                    component_labels[j] = componentId\n                    Q.append(j)\n            Q.popleft()\n        componentId += 1\n# for each pair of disconnected components find the edge w/ min distance to connect \ncomp_edges = []\nfor i in range(componentId-1):\n    for j in range(i+1,componentId):\n        i_nodes = np.where(component_labels == i)[0]\n        j_nodes = np.where(component_labels == j)[0]\n        btwn_comps_matrix = dist_matrix[np.ix_(i_nodes, j_nodes)]\n        min_dist_matrix_coords = np.unravel_index(btwn_comps_matrix.argmin(),btwn_comps_matrix.shape)\n        comp_edges.append(\n            [\n                [i,j], # edge: component ids\n                btwn_comps_matrix[min_dist_matrix_coords], # distance\n                [i_nodes[min_dist_matrix_coords[0]],j_nodes[min_dist_matrix_coords[1]]], # edge: node ids\n            ]\n        )\ncomp_edges = sorted(comp_edges, key=lambda i: i[1])\n\n# Kruskal's Algorithm - Find Min Spanning Tree to reconnect entire graph\ncomp_connectors = []\njoined_comp_labels = np.arange(componentId)\nfor edge in comp_edges:\n    if joined_comp_labels[edge[0][0]] != joined_comp_labels[edge[0][1]]:\n        joined_comp_labels[np.where(joined_comp_labels==joined_comp_labels[edge[0][1]])] = joined_comp_labels[edge[0][0]]\n        comp_connectors.append(edge[-1])\n\n# connect adjacency matrix\nconnected_adj_matrix = adj_matrix.copy()\nfor edge in comp_connectors:\n    connected_adj_matrix[edge[0], edge[1]] = 1\n    connected_adj_matrix[edge[1], edge[0]] = 1\nfig, ax = plt.subplots(figsize=(20,20))\nplt.imshow(pixels,cmap='binary',alpha=.1)\nfor i in range(len(comp_connectors)):\n    pixel_coords = nodes[comp_connectors[i]]\n    plt.plot(pixel_coords[:,1],pixel_coords[:,0],linewidth=5);\n\nIt’s time to partition the pipeline.\nn_clusters = 74  # number of regions\nward = AgglomerativeClustering(\n   n_clusters=74, linkage=\"ward\", connectivity=connected_adj_matrix\n)\nmodel = ward.fit(nodes)\nLet’s create a network from the partitioned regions. We’ll need to find which clusters are adjacent.\n# create centroids for each cluster\ncenters = []\nfor i in range(n_clusters):\n    centers.append(nodes[np.where(ward.labels_ == i)].mean(axis=0))\ncenters = np.array(centers)\n\n# find adjacent clusters\ndist_btwn_clusters = np.zeros((n_clusters,n_clusters))\nfor i in range(n_clusters-1):\n    for j in range(i+1,n_clusters):\n        i_nodes = np.where(ward.labels_ == i)[0]\n        j_nodes = np.where(ward.labels_ == j)[0]\n        dist_btwn_clusters[i,j] = dist_matrix[np.ix_(i_nodes, j_nodes)].min()\n        dist_btwn_clusters[j, i] = dist_btwn_clusters[i, j]\n\nclusters_adj_matrix = (dist_btwn_clusters &lt; 2)\nnp.fill_diagonal(clusters_adj_matrix, False)\nplt.figure(figsize=(18,16))\nplt.imshow(pixels,cmap='binary',alpha=0)\nplt.scatter(nodes[:,1], nodes[:,0], c=ward.labels_, s=0.5, cmap='jet',alpha=0.2)\nplt.scatter(centers[:,1], centers[:,0], c='k', s=30);\n\nedges = np.where(clusters_adj_matrix)\nfor i in range(len(edges[0])):\n    plt.plot(\n        [centers[edges[0][i],1],centers[edges[1][i],1]],\n        [centers[edges[0][i],0],centers[edges[1][i],0]],\n        c='k'\n    )\n\nAnd that’s it! As a bonus the source provided some oilspill data. Let’s check where they occur. Oilspill coordinates are in latitude and longitude so we’ll have to convert it to our image’s coordinates using a mercator projection. The function below is derived from https://en.wikipedia.org/wiki/Mercator_projection#Mathematics\ndef convertGeoCordsToPixel(\n        coordinates,\n        pixelWidth, \n        pixelHeight, \n        longLeftmostPixel, \n        longRightmostPixel, \n        latSouthmostPixel):\n    latitudes = coordinates[:,0]\n    longitudes = coordinates[:,1]\n\n    pixelperLongDegree = (pixelWidth / (longRightmostPixel - longLeftmostPixel))\n    entireWorldWidth = pixelperLongDegree * 360\n    globeRadius =  entireWorldWidth / (2 * np.pi)\n\n    x = globeRadius * np.pi / 180 * (longitudes - longLeftmostPixel)\n\n    latitudes_Rad = latitudes * np.pi / 180 \n    latSouthmostPixel_Rad = latSouthmostPixel * np.pi / 180 \n    yOffset = globeRadius / 2 * np.log((1 + np.sin(latSouthmostPixel_Rad)) / (1 - np.sin(latSouthmostPixel_Rad)))\n    y = (yOffset + pixelHeight) - globeRadius / 2 * np.log((1 + np.sin(latitudes_Rad)) / (1 - np.sin(latitudes_Rad))) \n    \n    return np.array([y, x]).T\n\noilspills = pd.read_csv('oilspills.csv')\nspill_latlong = oilspills[['Latitude','Longitude']].values\n\nspill_coords = convertGeoCordsToPixel(\n    spill_latlong,\n    pixels.shape[1],\n    pixels.shape[0], \n    -134.9, \n    -48.5, \n    42\n) \n\nplt.figure(figsize=(10,10))\nplt.imshow(pixels, cmap='binary', alpha=.2)\nplt.scatter(spill_coords[:,1], spill_coords[:,0], ec='k', alpha=.5);\n\nLooks close enough. Finally, let’s assign each spill to the closest cluster.\noilspills['cluster'] = ward.labels_[cdist(spill_coords, nodes).argmin(axis=1)]\nspill_labels = oilspills['cluster'].values.tolist()\nspills_counts = [spill_labels.count(i) for i in range(n_clusters)]\n\nplt.figure(figsize=(18,16))\nplt.xlim(0,img.shape[1])\nplt.ylim(-img.shape[0],0)\nplt.scatter(nodes[:,1], -nodes[:,0],c=ward.labels_, s=0.5, cmap='jet',alpha=0.5,zorder=-100000000)\nplt.scatter(centers[:,1], -centers[:,0],c='k', s=25,zorder=-10000000);\nfor i in range(len(centers)):\n    plt.annotate(spills_counts[i],(centers[i,1],-centers[i,0]),size=15,bbox=dict(boxstyle=\"circle,pad=0.3\", fc='w', ec=\"k\", lw=2),zorder=-i)\nplt.axis('off');"
  },
  {
    "objectID": "posts-misc/yachtdice/index.html",
    "href": "posts-misc/yachtdice/index.html",
    "title": "Searching for Johnny Yacht Dice",
    "section": "",
    "text": "Yacht is a game played with five dice. Players have twelve turns to score as many points as possible using the dice values to form hands. Each turn is played as such: 1. Roll all five dice 2. Select which dice to keep and reroll the rest. This action can be repeated once more. 3. Select a category used to score the hand and add it to your current point total. A player cannot select a category more than once a game.\nThere are 12 possible categories to select:\n\n\n\nSingle Digit/Bonus Hands\nScoring\n\n\n\n\nOnes\nOnly sum the dice that rolled a 1\n\n\nTwos\nOnly sum the dice that rolled a 2\n\n\nThrees\nOnly sum the dice that rolled a 3\n\n\nFours\nOnly sum the dice that rolled a 4\n\n\nFives\nOnly sum the dice that rolled a 5\n\n\nSixes\nOnly sum the dice that rolled a 6\n\n\n\n\n\n\n\n\n\n\nStandard Hands\nScoring\n\n\n\n\nChoice\nSum all dice values\n\n\nFull House\nIf three dice share the same value and the remaining two are the same sum all dice otherwise score 0\n\n\nFour of a Kind\nIf four or more dice share the same value sum all dice otherwise score 0\n\n\n\n\n\n\n\n\n\n\nBinary Hands\nScoring\n\n\n\n\nSmall Straight\nIf four dice form a sequence of consecutive numbers score 15 otherwise score 0\n\n\nLarge Straight\nIf all five dice form a sequence of consecutive numbers score 30 otherwise score 0\n\n\nYacht\nIf all dice have the same value score 50 otherwise score 0\n\n\n\nIf a player scores 63 or more points from the Bonus Hand Categories, they receive an additional 35 points.\nAfter a couple of games, ideas on optimal play will start to percolate in your head. Let’s explore them in python.\nimport itertools\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.gridspec import GridSpec\nFirst we’ll create a list of all possible rolls and record their scores for each of the 12 hands.\nrolls = list(''.join(i) for i in itertools.combinations_with_replacement('123456', r=5))\nn_rolls = len(rolls)\n\nhands = [\n    'ones','twos','threes',\n    'fours','fives','sixes',\n    'choice','fourKind','fullHouse',\n    'smallStraight','largeStraight','yacht'\n    ]\nn_hands = len(hands)\n\ndef evalFourKind(count):\n    return np.dot(countArray,range(1,7)) if (max(count) &gt;= 4) else 0\n\ndef evalFullhouse(count):\n    if max(count) == 5:\n        return np.dot(countArray,range(1,7))\n    elif (3 in count) & (2 in count):\n        return np.dot(countArray,range(1,7))\n    else:\n        return 0\n\ndef evalSmallStraight(count):\n    straights = [[1,1,1,1,7,7],[7,1,1,1,1,7],[7,7,1,1,1,1]]\n    for i in range(3):\n        if sum(count[j] &gt;= straights[i][j] for j in range(6)) &gt;= 4:\n            return 15\n    return 0      \n\ndef evalLargeStraight(count):\n    return 30 if ((count == [1,1,1,1,1,0]) or (count == [0,1,1,1,1,1])) else 0\n\ndef evalYacht(count):\n    return 50 if (5 in count) else 0\n\n# store scores recieved based on roll and hand selected\nbase_hand_value_matrix = np.zeros((n_rolls, n_hands))\nfor idx in range(n_rolls):\n    roll = rolls[idx]\n    # convert roll to array with first entry storing number of ones rolled...\n    countArray = [roll.count(str(i)) for i in range(1, 7)]\n    base_hand_value_matrix[idx] = [\n        *[countArray[i] * (i+1) for i in range(6)],\n        np.dot(countArray, range(1, 7)),\n        evalFourKind(countArray),\n        evalFullhouse(countArray),\n        evalSmallStraight(countArray),\n        evalLargeStraight(countArray),\n        evalYacht(countArray)\n    ] \n\nprint('There are {} possible rolls'.format(n_rolls))\n# test scoring functions\nidx = 20\nprint(f'hand = {rolls[idx]} \\n----')\nfor i in hands:\n    print(f'{i} = {base_hand_value_matrix[idx, hands.index(i)]}')\nThere are 252 possible rolls\nhand = 11166 \n----\nones = 3.0\ntwos = 0.0\nthrees = 0.0\nfours = 0.0\nfives = 0.0\nsixes = 12.0\nchoice = 15.0\nfourKind = 0.0\nfullHouse = 15.0\nsmallStraight = 0.0\nlargeStraight = 0.0\nyacht = 0.0\nFor each reroll we have to decide which dice to hold onto. Let’s create a list of all possible options.\n# let 1 indicate we keep the dice, 0 indicate we reroll it\nkeeps = list([''.join(i) for i in itertools.product('01',repeat=5)])\nn_keeps = len(keeps)\n\nprint('There are {} options during the reroll phase'.format(n_keeps))\nThere are 32 options during the reroll phase\nIt will be useful to know the probability of transitioning from one roll to another given which dice we choose to keep.\nWe can store the probabilities in a 252x32x252 matrix.\nstate_transition_matrix = np.zeros((n_rolls, n_keeps, n_rolls))\n\ndef frequencies(items):\n    counts = {}\n    for i in items:\n        counts[i] = counts.get(i, 0) + 1\n    return {k: v/len(items) for k, v in counts.items()}\n\n# dict containing all possible rolls using i dice\nfreq = {}\nfor num_to_reroll in range(6):\n    freq[num_to_reroll] = list(itertools.product('123456',repeat=num_to_reroll))\n\nfor state in range(n_rolls):\n    roll = rolls[state] # what is our current roll\n    for action in range(n_keeps):\n        dice_to_keep = keeps[action]  # select which dice to keep\n        kept_dice = ''.join(roll[i] for i in range(5) if dice_to_keep[i] == '1')\n        num_to_reroll = 5-len(kept_dice)\n        # get all possible new rolls with probability  \n        new_rolls = [''.join(sorted(kept_dice + ''.join(new_roll))) for new_roll in freq[num_to_reroll]]\n        f = frequencies(new_rolls)\n        # store in state_transition_matrix\n        for new_roll in f:\n           new_state = rolls.index(new_roll)\n           state_transition_matrix[state, action, new_state] = f[new_roll]\n\ninitial_roll_probs = state_transition_matrix[0, 0, :]\nLet’s check what the probability of moving from five ones to a full house, ones over sixes. To do this we keep 3 ones and reroll the rest. We know this is 1/36, which equals 0.02777.\ninit_state = rolls.index('11111')\nterm_state = rolls.index('11166')\naction = keeps.index('11100')\n\nprint(f'Probability of rolling \"11166\" from \"11111\" = {state_transition_matrix[init_state, action, term_state]}')\nProbability of rolling \"11166\" from \"11111\" = 0.027777777777777776\nNow let’s find the expected value of each hand prior to first roll, given we only make decisions to maximize said hand. We will need a policy to decide which dice to keep for two rerolls. The policy can be obtained using the transition probabilities and working backwards starting from the hand values after the last roll. We’ll use numpy’s einsum function to achieve this.\nRecall state_transition_matrix is a 252x32x252 matrix whose i,j,k dimensions indicate the initial roll, the 32 options to keep dice before rerolling, and the terminal roll. The entries correspond to the probability of landing on the terminal roll given which dice we chose to keep from the initial roll. The base_hand_value_matrix is a 252x12 matrix whose k,l dimensions indicate the terminal roll before we chose which of the 12 hands to select, and entries indicate the value of that selection. When we input ‘ijk,kl-&gt;ijl’ into einsum, we are informing the function to return a 252x32x12 matrix with entry i,j,l corresponding to the dot product of two 252 length vectors state_transition_matrix[i,j,:] and base_hand_value_matrix[:,l], or the expected value of our current roll after selecting which dice to keep and which hand to score. Now consider always taking the action which maximizes the expected value, we can reduce our 252x32x12 matrix output to a 252x12 matrix. This matrix, called ex_val_1_reroll_left, stores the expected value of each hand given our current state with one reroll left. We can repeat the procedure by replacing base_hand_value_matrix with ex_val_1_reroll_left to get the expected values with two reroll left. Finally we can multiply a vector probabilities for each roll at the start of the round by ex_val_2_reroll_left to obtain our solution.\nex_val_1_reroll_left = np.einsum(\n    'ijk,kl-&gt;ijl', \n    state_transition_matrix, \n    base_hand_value_matrix\n).max(axis=1)\n\nex_val_2_reroll_left = np.einsum(\n    'ijk,kl-&gt;ijl',\n    state_transition_matrix,\n    ex_val_1_reroll_left\n).max(axis=1)\n\nex_val_preroll = state_transition_matrix[0,keeps.index('00000'),:] @ ex_val_2_reroll_left\n\ndict(zip(hands,ex_val_preroll))\n{'ones': 2.106481481481479,\n 'twos': 4.212962962962959,\n 'threes': 6.319444444444442,\n 'fours': 8.425925925925918,\n 'fives': 10.532407407407414,\n 'sixes': 12.638888888888884,\n 'choice': 23.33333333333333,\n 'fourKind': 5.611263427672356,\n 'fullHouse': 7.013552612731233,\n 'smallStraight': 9.231634693554096,\n 'largeStraight': 7.83285050202343,\n 'yacht': 2.3014321262849484}\nMost of the time we will have more than one option when selecting a hand to score at the end of the round. We need to create a policy for all the possible combinations of available hands.\n# let 0 indicate the hand is available to score, 1 indicates it is unavailabile\navailable_hands = [''.join(i) for i in itertools.product('01',repeat=n_hands)][:-1]\nn_available_hands = len(available_hands)\nThere are 4095 possible senarios for which hands are still available in a game that has not ended.\nLet’s create a policy whose only concern is to maximize the score received at the end a turn.\n# useful for calcs\navailable_hand_matrix = np.zeros((n_available_hands, n_hands))\nfor a in range(n_available_hands):\n    ah = available_hands[a]\n    available_hand_matrix[a] = [np.NINF if i=='1' else 0 for i in list(ah)]\navail_hand_and_roll_indices = np.indices((len(available_hands),len(rolls)))\n\ndef create_simple_policy(hand_value_matrix):\n    '''Creates a greedy policy using a hand_value_matrix\n    Output : \n        state_policy_dict = {\n            0: (4095x252 matrix) w/ entries 0-11 referring to selecting an available hand at end of round\n            1: (4095x252 matrix) w/ entries 0-31 referring to which dice to keep at second reroll\n            2: (4095x252 matrix) w/ entries 0-31 referring to which dice to keep at first reroll\n        }\n    Globally Referenced Variables: state_transition_matrix, available_hand_matrix, avail_hand_and_roll_indices\n    '''\n    state_policy_dict = {}\n    state_value_dict = {}\n\n    # select hand at final step from those available which maximized score obtained in round:\n    b = hand_value_matrix[None,:,:]+available_hand_matrix[:,None,:]\n    state_policy_dict[0] = np.argmax(b,axis=2)\n    state_value_dict[0] =  b[(*avail_hand_and_roll_indices,state_policy_dict[0])]\n    \n    # calculate reroll policies and expected values:\n    # replace einsum with tensordot for speed improvement\n    a = np.tensordot(state_value_dict[0], state_transition_matrix, axes=((1),(2)))\n    state_policy_dict[1] = np.argmax(a,axis=2)\n    state_value_dict[1] =  a[(*avail_hand_and_roll_indices,state_policy_dict[1])]\n\n    a = np.tensordot(state_value_dict[1], state_transition_matrix, axes=((1),(2)))\n    state_policy_dict[2] = np.argmax(a,axis=2)\n    #state_value_dict[2] =  a[(*avail_hand_and_roll_indices,state_policy_dict[2])]\n\n    return state_policy_dict #, state_value_dict\n\nold_state_policy_dict = create_simple_policy(base_hand_value_matrix)\n%timeit create_simple_policy(base_hand_value_matrix)\n809 ms ± 63.7 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\nNow let’s check which actions our policy takes in certain scenarios.\nrerolls_left = 2\ninit_state = rolls.index('11123')\nah_idx = available_hands.index('000000000000')\naction = old_state_policy_dict[rerolls_left][ah_idx, init_state]\nprint(keeps[action])\n\nrerolls_left = 2\ninit_state = rolls.index('11123')\nah_idx = available_hands.index('011111111111')\naction = old_state_policy_dict[rerolls_left][ah_idx, init_state]\nprint(keeps[action])\n\nrerolls_left = 0\ninit_state = rolls.index('11123')\nah_idx = available_hands.index('000000000000')\naction = old_state_policy_dict[rerolls_left][ah_idx, init_state]\nprint(hands[action])\n00000\n11100\nchoice\nSo if we open the game rolling 1,1,1,2,3 our policy wants us to reroll everything. It also correctly determines to only keep ones when “ones” is the only hand left to score. Now the last piece of advice is alarming. When all hands are available it opts to score using choice. That gives us 8 out of a potential 30 points. If instead it opted for ones, it would leave just 2 points on the table. We’ve created a greedy algorithm!\nNow let’s build some agents and test out their prowess.\nclass yachtDiceAgent:\n    def __init__(\n        self, \n        state_policy_dict, \n        init_availHandsId=0, \n        init_game_score=0, \n        init_bonus_progress=0,\n        bonus_progress_required=63,\n        bonus_reward=35\n    ):\n        self.state_policy_dict = state_policy_dict\n        self.score_keeper = []\n        self.bonus_keeper = []\n        self.hand_keeper = []\n        self.init_availHandsId = init_availHandsId\n        self.init_game_score = init_game_score\n        self.init_bonus_progress = init_bonus_progress\n        self.bonus_progress_required = bonus_progress_required\n        self.bonus_reward = bonus_reward\n\n    def _rollDice(self, keep, diceRoll):\n        newRoll = ''\n        for i in range(5):\n            if keep[i] == '0':\n                newRoll += str(np.random.randint(1, 7))\n            else:\n                newRoll += diceRoll[i]\n        diceRoll = ''.join(sorted(newRoll))\n        diceRollId = rolls.index(diceRoll)\n        return diceRoll, diceRollId\n\n    def get_action(self, availHandsId, bonus_progress, rerolls_left, diceRollId):\n        return self.state_policy_dict[rerolls_left][availHandsId, diceRollId]\n\n    def simulate_game(self):\n        # initialize game\n        availHandsId = self.init_availHandsId\n        game_score = self.init_game_score\n        bonus_progress = self.init_bonus_progress\n        points = np.zeros(12)\n        turn = available_hands[availHandsId].count('1')\n        # play out game\n        while turn &lt; 12:\n            # initial roll\n            # note: there's a noticable improvement to preformance by assigning diceRoll to local vs. instance\n            diceRoll, diceRollId = self._rollDice(keeps[0], '11111')\n            # two rerolls \n            for rerolls_left in [2,1]:\n                action = self.get_action(availHandsId, bonus_progress, rerolls_left, diceRollId)\n                diceRoll, diceRollId = self._rollDice(keeps[action], diceRoll)\n            # select hand\n            action = self.get_action(availHandsId, bonus_progress, 0, diceRollId)\n            availHandsId += 2**(11-action)\n            # end turn\n            points[action] += base_hand_value_matrix[diceRollId, action]\n            turn += 1\n\n            # evaluate score\n            bonus_progress = int(min(self.bonus_progress_required, self.init_bonus_progress + sum(points[:6])))\n        game_score += sum(points) + self.bonus_reward*(bonus_progress &gt;= self.bonus_progress_required)\n        # record stats\n        self.score_keeper.append(game_score)\n        self.bonus_keeper.append(bonus_progress &gt;= self.bonus_progress_required)\n        self.hand_keeper.append(points)\n\n    def simulate_games(self, n_sims):\n        for i in range(n_sims):\n            self.simulate_game()\n\n    def summarize_results(self, name='', color='b', n_bins=40, max_score=323):\n        n_sims = len(self.score_keeper)\n        avg_score = np.mean(self.score_keeper)\n        avg_bonus = 100*np.mean(self.bonus_keeper)\n        hand_matrix = np.vstack(self.hand_keeper)\n\n        fig = plt.figure(figsize=(10,5))\n        gs = GridSpec(3, 6, figure=fig)\n        fig.add_subplot(gs[:, :3])\n        for i in range(9):\n            fig.add_subplot(gs[i//3, (i%3)-3])\n\n        fig.axes[0].set_xlabel('Possible Scores')\n        fig.axes[0].set_ylabel('Count')\n        fig.axes[0].set_title(\"{}'s {} games played\".format(name, n_sims))\n\n        fig.axes[0].hist(\n            self.score_keeper,\n            bins=np.linspace(0, max_score, n_bins),\n            alpha=0.5,\n            color=color,\n            edgecolor=color)\n        fig.axes[0].hist(\n            [self.score_keeper[i] for i in range(n_sims) if self.bonus_keeper[i] == 1],\n            bins=np.linspace(0, max_score, n_bins),\n            color=color,\n            edgecolor=color,\n            label='got bonus: {:.0f}%'.format(avg_bonus))\n\n\n        x = fig.axes[0].get_legend_handles_labels()\n        y = ['+ve {}: {:.0f}%'.format(hands[i][:5],100*np.mean(hand_matrix[:,i] != 0)) for i in range(7,12)]\n        y = ['avg score: {:.2f}'.format(avg_score)] + y\n        x = fig.axes[0].legend(\n            [x[0][0] for i in range(len(y)+1)], \n            [x[1][0],*y], \n            handlelength=0.7, \n            prop={'family':'monospace'}\n        )\n        for i in range(len(y)):\n            x.legendHandles[i+1].set(visible=False)\n\n        for i in range(1, 10):\n            fig.axes[i].hist(\n                hand_matrix[:,i-1]/(i if i&lt;=6 else 1),\n                bins=np.arange(-.5,6.5) if i&lt;=6 else np.arange(-0.5,31.5),\n                density=1,\n                histtype='bar' if i&lt;=6 else 'stepfilled',\n                alpha=0.5,\n                color=color,\n                edgecolor=color\n            )\n            fig.axes[i].set_title(hands[i-1])\n\n        plt.tight_layout()\n        #plt.savefig('{}'.format(name.replace(' ', '-').lower()), facecolor='white')\n        plt.show()\nclass randomYachtDiceAgent(yachtDiceAgent):\n    def __init__(self, state_policy_dict, init_availHandsId=0, init_game_score=0, init_bonus_progress=0, random_hands=True):\n        super().__init__(state_policy_dict, init_availHandsId, init_game_score, init_bonus_progress)\n        self.random_hands = random_hands\n\n    def get_action(self, availHandsId, bonus_progress, rerolls_left, diceRollId):\n        if rerolls_left == 0:\n            if self.random_hands:\n                x = [i for i in range(12) if available_hands[availHandsId][i] == '0']\n                return x[np.random.randint(len(x))]\n            else:\n                return self.state_policy_dict[0][availHandsId, diceRollId]\n        else:\n            return np.random.randint(32)\nOur first bot makes decisions completely at random.\nbot = randomYachtDiceAgent(old_state_policy_dict)\nbot.simulate_games(10000)\nbot.summarize_results(name='Random Agent', color='g')\n\nThis is a bit mean so we’ll update it to only randomize the rerolls so it can select the highest scoring hand at the end of the round.\nbot = randomYachtDiceAgent(old_state_policy_dict, random_hands=False)\nbot.simulate_games(10000)\nbot.summarize_results(name='Semi-Random Agent', color='g')\n\nOur next bot operates using the greedy policy we’ve created. It scores significantly higher than our random agent.\nbot = yachtDiceAgent(old_state_policy_dict)\nbot.simulate_games(10000)\nbot.summarize_results(name='Greedy Agent', color='b')\n\nAs previously addressed, greed is not always optimal. We need to teach the bots to weigh long term opportunities against cashing in on short term rewards. To do this we’ll adjust the behaviour of new agents by altering their preceived hand values prior to creating a greedy policy. Let’s make the next bot by subtracting the expected value of each hand at the start of the turn from it’s original value. It scores roughly 15 points higher on average.\nnew_hand_value_matrix = base_hand_value_matrix - ex_val_preroll\nbot = yachtDiceAgent(create_simple_policy(new_hand_value_matrix))\nbot.simulate_games(10000)\nbot.summarize_results(name='Expected Value Agent', color='r')\n\nOur new bot obtains the bonus roughly 10% of the time. Factoring the bonus into our decision making process is definetly worth exploring if we want to best our new average. Let’s try to make a bot that prioritizes reaching the bonus so we can see just how frequently one can obtain it. We’ll restrict ourselves to only manipulating the base hand values without including additional information like the current bonus progress in our transformations. This is suboptimal but keeps the policy creation time at a minimum.\nHere’s a naive approach. The bonus requires scoring a total of 63 points from all the single digit hands. 63 can be achieved by scoring at least a 3 count for each digit, 63 = 3*(1+2+3+4+5+6). Let’s transform the base hand values such that our bot overvalues anything higher than a 3 count and fears picking anything less.\nx = base_hand_value_matrix.copy()\n# punish anything below a 3 count for bonus hands\nx[x &lt; 3 * np.arange(1,13) * (np.arange(12) &lt; 6)] -= 100\n# reward anything above a 3 count for bonus hands\nx[x &gt;= 3 * np.arange(1,13) + 100*(np.arange(12) &gt;= 6)] += 100\n\nbot = yachtDiceAgent(create_simple_policy(x))\nbot.simulate_games(10000)\nbot.summarize_results(name='Bonus Agent', color='orange')\n\n# lets check the prob of getting at least a 3 count using the same method for finding the expected values but\n# convert base_hand_value_matrix into a binary matrix with ones only at acceptable results\n\nx = base_hand_value_matrix-base_hand_value_matrix.max(axis=0)*(3/5)&gt;=0\nx *= (np.arange(12) &lt; 6)\nprob_1_reroll_left = np.einsum('ijk,kl-&gt;ijl',state_transition_matrix,x.astype(int)).max(axis=1)\nprob_2_reroll_left = np.einsum('ijk,kl-&gt;ijl',state_transition_matrix,prob_1_reroll_left).max(axis=1)\nprob_preroll = state_transition_matrix[0,keeps.index('00000'),:] @ prob_2_reroll_left\nprob_preroll[0]\n0.35484999797783096\nThe bonus agent achieves it’s goal roughly 90% of the time. This might seem high if you consider the probability of getting a 3 count above but there’s a lot of mechanisms working in our favour. First, the bot uses the six non bonus hands to dump any undesirable results. Second, the probability above is calculated as if we chose the hand prior to seeing the initial roll for the round, in game we use the knowledge obtained from that roll to determine what to pursue. Finally, scoring higher than a 3 count can ease the minimum count required for other hands.\nLet’s try one last bot whose only desire is to pursue the perfect game.\n# lets check the prob of getting a max score for each hand using same method for finding the expected values but\n# convert base_hand_value_matrix into a binary matrix with ones only at acceptable results\n\nmask = base_hand_value_matrix-base_hand_value_matrix.max(axis=0)==0\nprob_1_reroll_left = np.einsum('ijk,kl-&gt;ijl',state_transition_matrix,mask.astype(int)).max(axis=1)\nprob_2_reroll_left = np.einsum('ijk,kl-&gt;ijl',state_transition_matrix,prob_1_reroll_left).max(axis=1)\nprob_preroll = state_transition_matrix[0,keeps.index('00000'),:] @ prob_2_reroll_left\ndict(zip(hands,prob_preroll))\n{'ones': 0.013272056011374675,\n 'twos': 0.01327205601137467,\n 'threes': 0.013272056011374662,\n 'fours': 0.013272056011374652,\n 'fives': 0.013272056011374659,\n 'sixes': 0.013272056011374652,\n 'choice': 0.013272056011374652,\n 'fourKind': 0.013272056011374652,\n 'fullHouse': 0.013272056011374652,\n 'smallStraight': 0.6154423129036065,\n 'largeStraight': 0.2610950167341143,\n 'yacht': 0.04602864252569902}\nbot = yachtDiceAgent(create_simple_policy(mask))\nbot.simulate_games(10000)\nbot.summarize_results(name='Perfect Game Agent', color='purple')\n\nso close…\nTo defeat our current champion we’ll need to spend more time thinking up which reward transformations can express potentially desireable behaviours. Let’s plot the expected value agent’s transformations. What kind of behaviour do you expect from the bot if it finished rolling with five sixes? Would you rather select 30 points and a chunk of bonus progress by picking “sixes” or cash in on 50 points from a yacht hand? What about “sixes” vs four of a kind? Are you more certain in your decision if you rolled five ones instead? What determines when we use a binary hand to dump an unwanted roll? How can we tinker with the rewards to answer these questions?\npossible_hand_scores = [np.unique(base_hand_value_matrix[:,i]) for i in range(12)]\n\nn_possible_values = 0\nfor i in range(12):\n    n_possible_values += len(possible_hand_scores[i])\nprint(f\"There's a total of {n_possible_values} values that can be manipulated to produce new policies.\")\nThere's a total of 120 values that can be manipulated to produce new policies.\ndef plot_reward_transformations(hand_value_matrix):\n    fig, ax = plt.subplots(1, 3, sharey=True, figsize=(12,6))\n    fig.subplots_adjust(wspace=0)\n    titles = ['Bonus Hands','Standard Hands','Binary Hands']\n    x_labels = ['counts','base hand value','achieved']\n    x_ticks = [np.arange(6),np.linspace(0,30,6),[0,1]]\n\n    fig.suptitle('Reward Transformations')\n    ax[0].set_ylabel('new hand value')\n    for i in range(12):\n        j = i//6 + i//9\n        denom = i+1 if j == 0 else 1 if j == 1 else max(possible_hand_scores[i])\n        ax[j].scatter(\n            possible_hand_scores[i]/denom,\n            sorted(np.unique(hand_value_matrix[:,i])), \n            label=hands[i])\n        ax[j].set_xticks(x_ticks[j])\n        ax[j].set_xlabel(x_labels[j])\n        ax[j].legend()\n        ax[j].set_title(titles[j])\n        \n    #plt.savefig('reward-transform', facecolor='white')\n\nplot_reward_transformations(new_hand_value_matrix)\n\nLet’s inspect how the bot orders selecting each available hand after rolling five sixes. We’ll also play out 1000 games after selecting each hand as if we rolled it on first turn.\nroll = '66666'\nvalues = base_hand_value_matrix[rolls.index(roll)]\nnew_values = new_hand_value_matrix[rolls.index(roll)]\nscores = []\n\n# create agent\nbot = yachtDiceAgent(create_simple_policy(new_hand_value_matrix))\nfor i in range(12):\n    # init game after first turn\n    bot.init_availHandsId = available_hands.index(''.join(['1' if k == i else '0' for k in range(12)]))\n    bot.init_game_score = values[i]\n    bot.init_bonus_progress = values[i]*(i &lt; 6)\n    bot.score_keeper = []\n    bot.simulate_games(1000)\n    scores.append(np.mean(bot.score_keeper))\n\nfig, ax = plt.subplots(figsize=(10, 4))\nax.set_title(\"First Turn diceRoll: {}\".format(roll), fontweight='bold')\nax.set_xlabel(\"average final score after selection in 1000 games\")\nax.yaxis.set_visible(False)\nax1 = ax.twiny() \nax1.set_ylim(-0.2, 3.2)\nax1.set_xlabel(\"bot's preceived hand value\")\nax1.plot(\n    sorted(new_values), \n    3*np.ones_like(new_values),\n    linewidth=1,\n    solid_capstyle='round',\n    color='k',\n    zorder=-1\n)\nax.plot(\n    sorted(scores),\n    np.zeros_like(scores),\n    linewidth=1,\n    solid_capstyle='round'\n    ,color='k',\n    zorder=-1\n)\nfor i in range(12):\n    temp_x = min(new_values) + (max(new_values) - min(new_values))*(scores[i] - min(scores)) / (max(scores) - min(scores))\n    ax1.plot(\n        [new_values[i], new_values[i], temp_x, temp_x],\n        [3, 2, 1, 0],\n        '-.' if i &gt; 5 else '-',\n        label=hands[i]\n    )\n    ax1.scatter(new_values[i], 3, marker=7 if i &gt; 5 else 6)\n    ax.scatter(scores[i], 0, marker=6 if i &gt; 5 else 7)\nax1.legend(\n    bbox_to_anchor=(1.01, .5),\n    loc='center left'\n);\n\nThis bot won’t pick sixes over full house or four of a kind but if we intervene to force sixes the bot ends up improving greatly. How can we transform the preceived hand values to accommodate this? Can we attempt to do this automatically?\nLet’s convert each hand value to it’s average final score by following the rungs on the ladder plot above. We’ll need to do this for every roll then repeat to see if we can converge on a final strategy.\nnum_games = 1000\nnum_iters = 5\nresults = {i: {'n_sims':num_games*(i+1),'avg_score':0} for i in range(num_iters)}\n\nnew_hand_value_matrix = base_hand_value_matrix.copy()\nbot = yachtDiceAgent(create_simple_policy(new_hand_value_matrix))\n\nfor t in results:\n    n_sims = results[t]['n_sims']\n    # keep newly calculated hand values here before updating hand_value_matrix\n    store_hand_value = {}\n\n    # bonus hands\n    for i in range(1,7):\n        store_hand_value[i] = []\n        # initiate sims from this gamestate (after first turn)\n        bot.init_availHandsId = available_hands.index(''.join(['1' if k == i else '0' for k in range(1,13)]))\n        bot.bonus_keeper = []\n        bot.score_keeper = []\n        bot.hand_keeper = []\n        bot.simulate_games(n_sims)\n\n        for j in range(6):\n            hand_matrix = np.vstack(bot.hand_keeper)\n            # add bonus progress achieved in simulation with bonus progress from initiated hand\n            bonus_prog = hand_matrix[:,:6].sum(axis=1) + np.tile(i*j,n_sims)\n            store_hand_value[i].append(\n                np.mean(\n                    bot.score_keeper + np.tile(i*j,n_sims) + np.where((bonus_prog &gt;= 63) != bot.bonus_keeper, 35, 0)\n                )\n            )\n\n    # non bonus hands\n    for i in range(7,13):\n        # initiate sims from this gamestate (after first turn with best possible score)\n        bot.init_availHandsId = available_hands.index(''.join(['1' if k == i else '0' for k in range(1,13)]))\n        bot.score_keeper = []\n        bot.simulate_games(n_sims)\n        store_hand_value[i] = possible_hand_scores[i-1] + np.mean(bot.score_keeper)\n\n    # replace hand values \n    new_hand_value_matrix = np.zeros_like(base_hand_value_matrix)\n    for i in store_hand_value:\n        for j in range(len(store_hand_value[i])):\n            new_hand_value_matrix[np.where(base_hand_value_matrix[:,i-1] == possible_hand_scores[i-1][j]), i-1] = store_hand_value[i][j]\n\n    # create and evaluate new bot\n    bot = yachtDiceAgent(create_simple_policy(new_hand_value_matrix))\n    bot.simulate_games(n_sims)\n    results[t]['avg_score'] = np.mean(bot.score_keeper)\n    print('{} in {} games'.format(results[t]['avg_score'], n_sims))\n\nbot.summarize_results(name='Contender', color='teal')\n181.365 in 1000 games\n184.879 in 2000 games\n186.61633333333333 in 3000 games\n187.674 in 4000 games\n186.2746 in 5000 games\n\nplot_reward_transformations(final_hand_value_matrix)\n\nWe’ve bested our previous record by roughly 7 points! By plotting the reward transformations, two changes become apparent. First, our procedure naturally incorporated the bonus value into the single digit hands. Second, the new agent will dump a bad round on a small straight before a large straight. It is also noteworthy that all the previous agents will converged to this final one (even the perfect game agent).\nIs this the best we can do? No, our bots are restricted to static hand values ignoring any information gained during the game. This is a pretty good result managed within 3 minutes. Let’s see what happens if we include available hands and bonus progress in the decision making process. We’ll acheive this in a similar manner except the new rewards at each state will be the sum of the current reward and expected value of total points from the resulting next state to the end of the game. We’ll be working backwards from the last round to the first calculating new policy along the way. This will take a long time if we’re not careful, I’ve managed to reduce it to roughly 10 minutes by combining states that will have identical strategies.\n# get available hands for each turn number \navailable_hands_in_turn = {i:[] for i in range(12)}\nfor i in range(n_available_hands):\n    available_hands_in_turn[available_hands[i].count('1')].append(i)\n\n# include bonus progress in representing state\n# determine all relevent bonus progress values for each available hands combo\n# relevent = changes strategy and is possible to obtain for said round \n\n# matrix to map any irrelevent states to relevent ones\nrelevent_bonus_mapping = np.zeros((n_available_hands, 64))\n\nrelevent_bonus_testing = {}\n\nfor i in range(n_available_hands):\n    avail_bonus_hands = [j+1 for j in range(6) if available_hands[i][j] == '0']\n    used_bonus_hands = [j for j in range(1,7) if j not in avail_bonus_hands]\n    need_to_test_progs = sorted(list(set([0] + [63 - min(63, sum(j)) for j in itertools.product(*[i*np.arange(6) for i in avail_bonus_hands])])))\n    maximum_possible_prog = min(63, 5*sum(used_bonus_hands))\n    relevent_bonus_testing[i] = [i for i in need_to_test_progs if i &lt;= maximum_possible_prog] \n    x = relevent_bonus_testing[i] + [64]\n    for j in range(len(x)-1):\n        relevent_bonus_mapping[i, x[j]:x[j+1]] = x[j]\n\n# include post final round\nrelevent_bonus_mapping = np.vstack([relevent_bonus_mapping, relevent_bonus_mapping[4094]])\n\nprint(f'number of states to calculate: {sum(len(relevent_bonus_testing[i]) for i in relevent_bonus_testing)} out of {n_available_hands*64}')\nnumber of states to calculate: 119870 out of 262080\n# initiate policy dict\nstate_policy_dict = {}\nfor i in range(3):\n    state_policy_dict[i] = np.transpose(np.repeat(old_state_policy_dict[i][:,:,np.newaxis], 64, axis=2), axes=[0,2,1])\n\nBranchingStates = {}\n# end game calc bonus\nBranchingStates[4095,0] = 0\nBranchingStates[4095,63] = 35\n\nfor turn in np.arange(11,-1,-1):\n    print(turn)\n    for ah_idx in available_hands_in_turn[turn]:\n        for bp_idx in relevent_bonus_testing[ah_idx]:\n            # make new policy by imputing hand values\n            x = np.zeros_like(base_hand_value_matrix) - 1\n            hand_options = [i for i in range(12) if available_hands[ah_idx][i] == '0']\n            for h in hand_options:\n                next_ah_idx = ah_idx + 2**(11-h)\n                for p in possible_hand_scores[h]:\n                    next_bp_idx = int(min(63, bp_idx + p*(h &lt; 6)))\n                    adj_next_bp_idx = relevent_bonus_mapping[next_ah_idx, next_bp_idx]\n                    x[np.where(base_hand_value_matrix[:,h]==p),h] = (p + BranchingStates[next_ah_idx, adj_next_bp_idx])\n\n            # get all affected bonus progresses\n            y = sum(1 for j in relevent_bonus_testing[ah_idx] if bp_idx &gt;= j) \n            if y &gt;= len(relevent_bonus_testing[ah_idx]):\n                z = 64\n            else:\n                z = relevent_bonus_testing[ah_idx][y]\n\n            # make new policy and calculate expected value of state\n            state_policy_dict[0][ah_idx, bp_idx:z, :] = np.argmax(x, axis=1)\n            value_0 = x.max(axis=1)\n            a = np.einsum('i,jki-&gt;jk',value_0, state_transition_matrix)\n            state_policy_dict[1][ah_idx, bp_idx:z, :] = np.argmax(a, axis=1)\n            value_1 = a.max(axis=1)\n            a = np.einsum('i,jki-&gt;jk',value_1, state_transition_matrix)\n            state_policy_dict[2][ah_idx, bp_idx:z, :] = np.argmax(a, axis=1)\n            value_2 = a.max(axis=1)\n            BranchingStates[ah_idx, bp_idx] = value_2 @ initial_roll_probs\n\nprint(f'This bot\\'s expected score is {BranchingStates[0,0]}')\nThis bot's expected score is 191.77436918834172\nplot_reward_transformations(x)\n\nThe final bot’s first round reward transformations look very similar to our previous contender.\nclass newYachtDiceAgent(yachtDiceAgent):\n    def __init__(self, state_policy_dict, init_availHandsId=0, init_game_score=0, init_bonus_progress=0):\n        super().__init__(state_policy_dict, init_availHandsId, init_game_score, init_bonus_progress)\n\n    def get_action(self, availHandsId, bonus_progress, rerolls_left, diceRollId):\n        return self.state_policy_dict[rerolls_left][availHandsId, bonus_progress, diceRollId]\nnew_bot = newYachtDiceAgent(\n    state_policy_dict,\n)\nnew_bot.simulate_games(10000)\nnew_bot.summarize_results(name='Johnny Yacht Dice', color='royalblue')\n\nOne last thing, we can calculate the probability of getting a perfect game now.\nperfect_hand_value_matrix = (base_hand_value_matrix-base_hand_value_matrix.max(axis=0)==0).astype(int)\nperfect_policy_dict = {}\nfor i in range(3):\n    perfect_policy_dict[i] = old_state_policy_dict[i].copy()\n\nBranchingPerfects = {}\nBranchingPerfects[4095] = 1\n\nfor turn in np.arange(11,-1,-1):\n    for ah_idx in available_hands_in_turn[turn]:\n        x = np.zeros_like(base_hand_value_matrix) - 1\n        hand_options = [i for i in range(12) if available_hands[ah_idx][i] == '0']\n\n        for h in hand_options:\n            x[:,h] = 0\n            next_ah_idx = ah_idx + 2**(11-h)\n            x[np.where(perfect_hand_value_matrix[:,h]==1),h] = 1*BranchingPerfects[next_ah_idx]\n\n        # make new policy and calculate expected value of state\n        perfect_policy_dict[0][ah_idx, :] = np.argmax(x, axis=1)\n        value_0 = x.max(axis=1)\n        a = np.einsum('i,jki-&gt;jk',value_0, state_transition_matrix)\n        perfect_policy_dict[1][ah_idx, :] = np.argmax(a, axis=1)\n        value_1 = a.max(axis=1)\n        a = np.einsum('i,jki-&gt;jk',value_1, state_transition_matrix)\n        perfect_policy_dict[2][ah_idx, :] = np.argmax(a, axis=1)\n        value_2 = a.max(axis=1)\n        BranchingPerfects[ah_idx] = value_2 @ initial_roll_probs\n        \nprint(f'It will take on average {1/BranchingPerfects[0]} games to achieve a perfect one')\nIt will take on average 276939929515183.94 games to achieve a perfect one"
  },
  {
    "objectID": "posts-hockey/prospect-models/part-1/index.html",
    "href": "posts-hockey/prospect-models/part-1/index.html",
    "title": "Part 1",
    "section": "",
    "text": "Bookmarking the current version of my prospect model by arriving at a trivial lesson. Assuming familiarity with the problem let’s jump right to my initial approach, a state space model.\nWe model player \\(i\\)’s point production for \\(GP\\) games played at position \\(k\\) in league \\(j\\) during season \\(t\\) at \\(u\\) years relative to draft eligibility as:\n\\[ Pts_{ijkt} \\sim \\mathbf{Poisson}(GP_{i,j,t} * \\lambda_{i,j,t}) \\]\n\n\\(\\log{\\lambda_{i,j,t}} = x_{i,t} + League_{j,t} + Age_{k,u}\\)\n\\(x_{i,t} = \\phi x_{i,t-1} + \\epsilon_{i,t}\\)\n\\(League_{j,t} = League_{j,t-1} + \\omega_{j,t}\\)\n\\(Age_{k,u} = Age_{k,u-1} + \\psi_{k,u}\\)\n\\(\\epsilon_{i,t} \\sim \\mathbf{Normal}(0, \\sigma^2)\\)\n\\(\\omega_{j,t} \\sim \\mathbf{Normal}(0, \\delta^2)\\)\n\\(\\psi_{k,u} \\sim \\mathbf{Normal}(0, \\gamma^2)\\)\n\\(\\phi \\in [0,1]\\)\n\nwhere a player’s ability \\(x_{i,t}\\) is modelled as a autoregressive process of order 1 across seasons.\nAs \\(\\phi\\) nears zero we do not expect ability to correlate between seasons (\\(x_t \\approx \\epsilon_t\\)). Player transfers become uninformative and the league coefficients collapse in order of their point rates.\nIf you constrain \\(\\sigma\\) to be small and \\(\\phi = 1\\) then a player’s ability remains static year to year (\\(x_t \\approx x_{t-1}\\)). This persistence gives the model footing for comparison and the league coefficients shift in accordance to what we see from historic drafting.\nThe issue I’ve come across is fitting a process flexible enough across an individual’s career while sufficiently rigid for evaluating leagues. It’s fairly relevant for this years NHL draft. Shaefer would jump to first in my rankings should we only consider this year’s 22 points in 17 games. But ignoring the 17 in 56 2023 campaign seems inconsistent when that is the kind of thing we rely on for calculating league equivalency!\nSeperating these goals into a two step model might be the obvious / correct / foolproof solution… but without deviating from my mess a quick possible fix is to fit\n\\[x_{i,t} = \\begin{cases}\n\\phi_1 x_{i,t} + \\epsilon_{1,i,t} & u\\le0 \\\\\n\\phi_2 x_{i,t} + \\epsilon_{2,i,t} & u\\gt0 \\\\\n\\end{cases}\\]\nwhere \\(u=0\\) is a player’s draft year and \\(\\epsilon_{1}\\) and \\(\\epsilon_{2}\\) correspond with two different \\(\\sigma\\)’s. That way I can let ability evolve as squiggly as necessary up until the day we submit picks and then stiffen up which could be more useful for comparing leagues.\nGood grief. Here’s how my current top 32 looks so far. Still loves the Q a little too much.\n Ridge plot ordering the top 27 eligible players for the 2025 NHL Draft based on the posteriors for player ability \\(x_{i,t}\\). In list form below.\n\n\n\nRank.\nLastName\nx\nNHL Scouting Rank\n\n\n\n\n1.\nMisa\n2.18\n2\n\n\n2.\nHagens\n2.06\n3\n\n\n3.\nSchaefer\n2.04\n1\n\n\n4.\nMartone\n2.01\n6\n\n\n5.\nFrondell\n1.94\n1 Int.\n\n\n6.\nAitcheson\n1.91\n9\n\n\n7.\nO’Brien\n1.86\n4\n\n\n8.\nDesnoyers\n1.85\n7\n\n\n9.\nEklund\n1.84\n2 Int.\n\n\n10.\nCarbonneau\n1.81\n16\n\n\n11.\nKindel\n1.8\n21\n\n\n12.\nReschny\n1.78\n25\n\n\n13.\nReid\n1.76\n23\n\n\n14.\nBear\n1.76\n10\n\n\n15.\nVeilleux\n1.74\n91\n\n\n16.\nZonnon\n1.71\n31\n\n\n17.\nMrtka\n1.71\n5\n\n\n18.\nSmith\n1.68\n13\n\n\n19.\nMartin\n1.66\n11\n\n\n20.\nSchmidt\n1.65\n43\n\n\n21.\nWoo\n1.64\n156\n\n\n22.\nHensler\n1.62\n12\n\n\n23.\nTremblay\n1.61\n\n\n\n24.\nMcQueen\n1.6\n8\n\n\n25.\nSpence\n1.6\n17\n\n\n26.\nBrisson\n1.57\n\n\n\n27.\nLakovic\n1.57\n14"
  },
  {
    "objectID": "posts-hockey/player-positions/index.html",
    "href": "posts-hockey/player-positions/index.html",
    "title": "Correcting Player Positions",
    "section": "",
    "text": "NHL clubs will commonly ice a roster consisting of 18 skaters; 4 lines from 12 forwards F and 3 pairs of defenders D. A Center C, Left Wing LW, and Right Wing RW form a line. Left Defence LD and Right Defence RD form a pair. During the even strength portions of a game a team will have 5 skaters on the ice. Predominantly one from each position. While some teams opt to tinker with this set up, we expect the total number of C, LW and RW games played to be balanced. For the 2021/2022 regular season we see this is not the case.\nPositions are hardly ever updated once a player joins the league. This leads to some untrustworthy situations. Consider the game between the Ducks and Hawks on March 23rd 2022 (gameId: 2021021018). By the submitted roster sheet, Anaheim played with 1 left wing, 1 right wing and a cast of 10 centers. On top of that, the league doesn’t distinguish between left and right defenders. It is commonly brought into question how well a defender fairs playing on their off hand, or how shallow the league’s RD depth is. Corrected position labels can settle these questions and other roster considerations. Our goal is to assigned one of the five positions to each skater observed in each game for the 2021/2022 regular season."
  },
  {
    "objectID": "posts-hockey/player-positions/index.html#forwards-vs.-defenders",
    "href": "posts-hockey/player-positions/index.html#forwards-vs.-defenders",
    "title": "Correcting Player Positions",
    "section": "Forwards vs. Defenders",
    "text": "Forwards vs. Defenders\nDefine the n by n matrix \\(\\Delta\\) whose \\((i, j)\\)-th entry corresponds to the number of seconds teammates \\(i\\) and \\(j\\) shared on the ice at even strength in a game. The diagonal entries refer to each player’s total ice time. \\(\\Delta\\) can be constructed using the shift data. Let \\(B\\) be the binary matrix obtained from the shift chart (refer to above image), then \\(\\Delta=BB^T\\). From \\(\\Delta\\), we compute \\(\\delta\\) by dividing each row by its diagonal entry, so each entry refers to the proportion of a player’s time spent with another. For convenience we set the diagonal elements of \\(\\delta\\) to zero. Each player is always accompanied by 4 teammates so every row will sum to 4. Following this thought, consider just the columns corresponding to defenders, \\(\\delta^D\\). The sum of a forward’s row will be very close to 2; and very close to 1 for a defender. Then the binary vector \\(x\\) which minimizes ${i=1}^{n} | 2-x_i-^D{i}x | $ exhibits our F/D labels (\\(x_i = 1\\) indicating the skater is a defender). I’ve opted to solve this heuristically, via a steepest ascent local search. Neighbourhoods are defined by at most 2 label mutations from the current candidate solution. While the search is robust to choice of starting location, the provided D and F labels are quite reliable.\n Tampa Bay’s shift data for the opening game of the 2021/2022 season presented in matrix form\n72 instances required swapping defenders to forwards, none in the other direction. One was the result of Robert Burtuzzo moving from D to F after the first period - caught with a row sum near 1.6. Which prompts the question, how will we handle mid game role changes? For simplicity, and issues detailed ahead, we restrict the problem to one label per player in each game."
  },
  {
    "objectID": "posts-hockey/player-positions/index.html#left-vs.-right-defenders",
    "href": "posts-hockey/player-positions/index.html#left-vs.-right-defenders",
    "title": "Correcting Player Positions",
    "section": "Left vs. Right Defenders",
    "text": "Left vs. Right Defenders\nWe proceed in a similar fashion to classical expectation-maximization algorithms for mixture models. EM is an iterative algorithm that is useful for handling missing data. While defender labels are truly missing, the forwards are so messed up it’s in our best interest to treat them so.\nDefenders are assigned an initial label using a biased coin, 51% in favour of whichever side of the \\(y = 0\\) line they appeared more often on in the game. This gentle nudge is only necessary to avoid any manual aliasing afterwards. The algorithm is described below in detail. In the next section we will adjust it to cluster the forwards.\n\nStep 1. Naive Bayes Classifier\nLet \\(x_{ij}\\) be a vector encoding the counts of each action \\(a\\) at each coordinate \\((x,y)\\) conducted by player \\(i\\) in game \\(j\\). For example, in the \\(j\\)-th game of the season, player \\(i\\) recorded one shot at center ice, thus \\(x_{ijk}=1\\) where \\(k=(\\text{Shooter},(0,0))\\) otherwise 0. We can think of each \\(x_{ijk}\\) as generated independently from a poisson distribution with mean \\(n_{ij}\\theta_{pk}\\) where \\(p\\) identifies the player’s position. Parameter \\(\\theta_{pk}\\) defines the rate at which \\(k\\) occurs per 60 minutes. \\(n_{ij}\\) is the constant exposure or player \\(i\\)’s ice time in game \\(j\\) divided by 60 minutes. Then we can model the entire population of defenders as a multivariate poisson mixture with two components for LD and RD.\nThe conditional likelihood of \\(x_{ij}\\) given it has label LD is:\n\\[f(x_{ij} | p=LD) = \\prod_{k=1}^K \\frac{(n_{ij}\\theta_{LD,k})^{x_{ijk}}e^{-n_{ij}\\theta_{LD,k}}}{x_{ijk}!}\\]\nFollowing bayes rule the posterior likelihood is:\n\\[f(LD | x_{i,j}) \\propto f(LD) f(x_{ij} | LD)\\]\nwhere \\(f(LD)\\) is the prior probability. Prior to any available information, both positions are considered to be equally likely, so \\(f(RD) = f(LD) = .5\\).\nFinally, the log posterior odds of the two positions is:\n\\[\n\\begin{aligned}\n\\alpha_{i,j} &= \\log \\frac{f(LD | x_{i,j})}{f(RD | x_{i,j})} \\\\\n&= \\sum x_{ijk} \\log \\frac{\\theta_{LD,k}}{\\theta_{RD,k}} + n_{i,j} \\sum(\\theta_{RD,k} - \\theta_{LD,k})\\\\\n\\end{aligned}\n\\]\nwhich determines membership for each sub-population. A positive \\(\\alpha_{i,j}\\) means the player is more likely to be LD.\nThe log odds require estimating the rate parameters, which in itself calls for existing labels. A typical solution using EM tumbles between the two calculations, with each update hopefully bringing us closer to convergence. At this point we’ve only utilized event data. A satisfying solution will pair both event and shift sources, and we address that in step 2.\n\n\nEstimating the rate parameters\nIt can be helpful to visualize the \\(\\theta_{p,k}\\)’s on a set of grids for each action, seen previously with the contour plots.\n\nFor each position and action store the total counts in matrix \\(M_p\\), whose rows and columns coincide with the (x,y) coordinates.\n\n\n\n\n\n\n\nFaceoffs are omitted for defenders. None were recorded, nor do we except them to hold any information for the position.\n\n\n\n\nAdd the matrix corresponding to its mirrored position flipped along the \\(y=0\\) entries to \\(M_p\\). Add corresponding total exposure times \\(N_p\\) as well. \\[M_{LD}'[x,y]=M_{LD}[x,y] + M_{LD}[x,-y]\\]\n\\[N_{LD}'=N_{LD} + N_{RD}\\]\n\n\n\n\n\n\n\nThis step enforces symmetry between the two positions and sets any action on the \\(y=0\\) line to have zero sway. Now if a player takes two shots at \\((x,-10)\\) and \\((x,10)\\) they will cancel each other out. It has the added benefit of equating the total rates for each action, setting the terms \\(\\sum(\\theta_{RD,k} - \\theta_{LD,k}) = 0\\). If we were given an event at an undisclosed location it provides no evidence. This is a wanted consequence, as it prevents clustering based on archetypes. For example “stay at home” defenders tend to have high hit and block rates but low shooting rates. The opposite is true for “offensive” defenders.\n\n\n\n\nApply a Kernel Smoothing Method to each matrix.\n\n\n\n\n\n\n\nTo avoid zero frequency problems we add a pseudo count of \\(\\frac{N_p}{N_{\\text{base class}}}\\) to each count (The fraction preserves a one to one ratio after calculating rates). Afterwards a kernel smoothing method is applied to \\(M_p'\\) to incorporate spatial dependency amongst \\(\\theta\\)’s. The main idea here is to apply regularization techniques, commonly found in generalized additive models, while retaining the benefits of our naive bayes classifier, predominantly speed.\nAt the moment I’ve using a gaussian kernel with \\(\\sigma = 5\\) based on visual inspection. A more principled approach, such as selection through cross validation, is left to future work. However, current results have been impartial to alternative choices. I’ve included a viz for \\(\\sigma = 20\\) at the bottom of the article. It will be worth looking into adaptive kernels, whose bandwidth fluctuates to accommodate sparse regions of the rink. Recall our log odds formula; our aim isn’t an accurate estimate of each position’s rate, but of the ratio between classes. It is the ratio which dictates the separation of classes. When count data is sparse for either position the estimated ratio can be highly variable and our pseudo count - which in some way influences the results like a prior - may have a stronger than intended effect.\n\n\n\n\nObtain the rates per 60 minutes by dividing each count by exposure \\(N_{p}'\\)\n\n\n\nStep 2. Pairwise Comparisons\nDue to the nature of the problem, whenever two teammates are on the ice we wish to compare them to determine who gets what label. The Bradley-Terry model is a standard approach to model pairwise comparisons. It’s commonly used in sports to model team strength from W-L records. The Davidson model is an extension to incorporate ties. Here we’ll treat winning as being assigned LD while sharing the ice with a RD. A tie refers to both players sharing the same label. The probabilities for each outcome is given as:\n\n\\[ p(i=LD, j=RD) = \\frac{e^{\\alpha_i}}{e^{\\alpha_i}+e^{\\alpha_j}+e^{z + .5(\\alpha_i+\\alpha_j)}}\\]\n\\[ p(i=j) = \\frac{e^{z + .5(\\alpha_i+\\alpha_j)}}{e^{\\alpha_i}+e^{\\alpha_j}+e^{z + .5(\\alpha_i+\\alpha_j)}}\\]\n\\[ p(i=RD, j=LD) = \\frac{e^{\\alpha_j}}{e^{\\alpha_i}+e^{\\alpha_j}+e^{z + .5(\\alpha_i+\\alpha_j)}}\\]\n\nWhere \\(z \\in (-\\infty,\\infty)\\) determines the probability of a tie. We set \\(z=0\\) which seems reasonable enough for our purposes. \\(\\alpha_i\\) represents player \\(i\\)’s “strength” or evidence for playing on the left side. The \\(\\alpha\\)’s are usually fit to the response data, but for this clustering problem we take them to be the log odds computed previously. I think I’m getting away with this since both the Davidson strength terms and log odds are relative values.\nSince pairwise comparisons only occur between teammates and only across one game, we can compartmentalize the labelling process accordinglyy. We find labels \\(l\\) such that log likelihood of our pairwise comparison model weighted by the time each pair spent on the ice is maximized:\n\\[ \\sum_{i&lt;j}\\Delta[i,j]*log(p(i=l_i, j=l_j))\\]\nFurthermore, we impose a constraint on \\(l\\) such that the size of the majority can only surpass the minority class by 1. This enforces our expectations of roster construction. The most defenders a team has iced this season is 7; which has 70 possible configurations of LD & RD. This makes brute force is a viable option.\nStep 1 and 2 are repeated until the labels converge or capped after a certain number of iterations.\n The histogram for strength terms between left and right defenders.\n\n\nIncorporating Correlation\nEvent data is like splicing a video stream into snapshots of key action moments. Key moments - which might be less discriminative than the times a player is simply floating around - are rare enough a single game’s worth doesn’t ensure success. The most extreme example being Buffalo’s entire D core registering a single even strength event in (gameId: 2021020566); Boston’s Curtis Lazar was generous enough to hit Casey Fitzgerald. Conversely, events can be misleading. It’s not uncommon for players to have “off” games while their team does not provide enough to mitigate the faulty evidence.\nThe good news is that we don’t have to restrict ourselves to one game worth of information. It is reasonable to expect that players prefer the same roles throughout the season. Say a pair play 20 games in one orientation, how much evidence do you need to be convinced they swapped roles for the next game? To stay within our framework, I’ve decided to apply a weighted aggregate to feature vectors. Each \\(x_{i,j} = \\sum_{i',j'}w_{i',j'}x_{i,j}\\) where \\(w\\) should resemble our intuition of which responses are correlated. This leads to a bevy of options. For example, a weighting can be proportional to the ice time given to the player in each game. This results in players having a constant \\(\\alpha\\) throughout the season. It would be equivalent to a set ranking for role priority. Another option is to only weigh in games where the player spends the majority of the time with common linemates. Lines can be determined by a graph constructed from \\(\\delta\\). Let an edge \\({i,j}\\) exist if \\(\\delta[i,j]\\) and \\(\\delta[j,i]\\) are greater than some threshold. Whittle down the threshold until you like what you see. Then all disconnected components which are triangles form F lines, all arcs form D pairs. For extra measure, restrict labels to avoid conflicts within each component.\n Histogram of log odds for 4 prominent members of Toronto’s Defence. Notice Morgan Rielly, who I contest played every game as LD strictly due to seniority, has some games suggesting otherwise. His partner for most of the year, TJ Brodie is flexible but indulges in the left side whenever the pair is split.\nLet’s compare the results Toronto’s defence with and without aggregating. I prefer the latter, which exhibits a cleaner separation at the cost of paving over any edge cases.\n\n\n\n\nSingle Game Strength\nSeason Strength Aggregate\nLinemate Strength Aggregate\n\n\nName\nLD\nRD\nα\nLD\nRD\nLD\nRD\n\n\nJake Muzzin\n47\n0\n3.57\n47\n0\n47\n0\n\n\nKristians Rubins\n3\n0\n3.3\n3\n0\n3\n0\n\n\nMark Giordano\n19\n1\n2.97\n20\n0\n19\n1\n\n\nMorgan Rielly\n76\n6\n2.32\n82\n0\n82\n0\n\n\nCarl Dahlstrom\n3\n0\n2.07\n3\n0\n3\n0\n\n\nRasmus Sandin\n43\n8\n1.25\n49\n2\n49\n2\n\n\nTravis Dermott\n23\n20\n0.32\n19\n24\n20\n23\n\n\nTJ Brodie\n28\n54\n0.31\n23\n59\n23\n59\n\n\nIlya Lyubushkin\n2\n29\n-1.47\n0\n31\n1\n30\n\n\nTimothy Liljegren\n3\n58\n-2.08\n0\n61\n0\n61\n\n\nJustin Holl\n0\n69\n-2.85\n0\n69\n0\n69\n\n\nAlex Biega\n0\n2\n-3.84\n0\n2\n0\n2"
  },
  {
    "objectID": "posts-hockey/player-positions/index.html#centers-vs.-wingers",
    "href": "posts-hockey/player-positions/index.html#centers-vs.-wingers",
    "title": "Correcting Player Positions",
    "section": "Centers vs. Wingers",
    "text": "Centers vs. Wingers\nSome slight adjustments are made to accommodate forwards. At each pass of the algorithm forwards are clustered into center and winger groups, then wingers into left and right. LW and RW labels are assigned similarly to defenders. Separating out centers is a bit different and we detail the changes below.\n\nFaceoffs are included, but since they only occur in 9 locations we do not apply any KDE, though there is merit to jacking up the pseudo count to balance its effect against the other smoothed over actions.\nWhen reflecting the counts, mirror center counts onto themselves.\nNormalize the count rates for each action except Faceoffs. Essentially this creates a debt which centers can easily work out of by taking faceoffs. Thus if no actions are taken, the player is more likely to be a winger.\nCalculate the log odds between center and the two winger classes as:\n\n\\[\\begin{aligned}\n\\beta_{i,j} &= log \\frac{f(C | x_{i,j})}{f(LW | x_{i,j})+f(RW | x_{i,j})} \\\\\n&= log\\frac{\\prod e^{-n_{i,j}\\theta_{C,k}}(n_{i,j}\\theta_{C,k})^{x_ijk}}{\\prod e^{-n_{i,j}\\theta_{LW,k}}(n_{i,j}\\theta_{LW,k})^{x_ijk} + \\prod e^{-n_{i,j}\\theta_{RW,k}}(n_{i,j}\\theta_{RW,k})^{x_ijk}}\\\\\n&= \\sum x_{ijk}log \\theta_{C,k} - n_{i,j} \\sum\\theta_{C,k} - log (\\prod e^{-n_{i,j}\\theta_{LW,k}}\\theta_{LW,k}^{x_ijk} + \\prod e^{-n_{i,j}\\theta_{RW,k}}\\theta_{RW,k}^{x_ijk})\\\\\n&= \\sum x_{ijk}log \\theta_{C,k} - n_{i,j} \\sum\\theta_{C,k} - log (\\prod e^{-n_{i,j}\\theta_{LW,k}}(\\prod\\theta_{LW,k}^{x_ijk} + \\prod\\theta_{RW,k}^{x_ijk}))\\\\\n&= \\sum x_{ijk}log \\theta_{C,k} - n_{i,j} \\sum(\\theta_{C,\\text{Faceoffs}} - \\theta_{LW,\\text{Faceoffs}}) - log (e^{\\sum x_{ijk}log \\theta_{LW,k}} + e^{\\sum x_{ijk}log \\theta_{RW,k}})\\\\\n\\end{aligned}\n\\]\n\nCalculate the log likelihood using the Davidson-Luce Model for groupwise comparisons [1]. Compare every combination of three forwards. Once again weigh by shared ice time. Limit the amount of C labels assigned in one game between \\(\\lfloor \\frac{\\text{\\# of forwards}}{3}\\rfloor\\) and \\(\\lceil \\frac{\\text{\\# of forwards}}{3}\\rceil\\).\n\n\\[ p(l_i=C, l_j=W, l_k=W) \\propto e^{\\beta_i}\\]\n\\[ p(l_i=W, l_j=C, l_k=W) \\propto e^{\\beta_j}\\]\n\\[ p(l_i=W, l_j=W, l_k=C) \\propto e^{\\beta_k}\\]\n\\[ p(l_i=C, l_j=C, l_k=W) \\propto e^{z_2 + \\frac{1}{2}(\\beta_i + \\beta_j)}\\]\n\\[ p(l_i=C, l_j=W, l_k=C) \\propto e^{z_2 + \\frac{1}{2}(\\beta_i + \\beta_k)}\\]\n\\[ p(l_i=W, l_j=C, l_k=C) \\propto e^{z_2 + \\frac{1}{2}(\\beta_j + \\beta_k)}\\]\n\\[ p(l_i=W, l_j=W, l_k=W) \\propto e^{z_3 + \\frac{1}{3}(\\beta_i + \\beta_j + \\beta_k)}\\]\n\\[ p(l_i=C, l_j=C, l_k=C) \\propto e^{z_3 + \\frac{1}{3}(\\beta_i + \\beta_j + \\beta_k)}\\]\n\n\n The histogram for log odds between center and winger. The mixture clearly exhibits two components, it’s easy to spot the pretenders immediately. The final distribution for centers is wide due to the weight of faceoffs. Modelling faceoff rates as a poisson distribution may not be the best choice, since coaches try to allocate them based on preference.\n The histogram for log odds between left and right wingers, a sad site.\nOnce again, let’s look at the results for Toronto. The winger labels are suspect until we aggregate games. Season and linemate weightings tend to agree. When they do not, manual inspection typically favours the latter when the line has played plenty of games together.\n\n\n\n\nSingle Game Strength\nSeason Strength Aggregrate\nLinemate Strength Aggregrate\n\n\nName\nC\nLW\nRW\nΒ\nα\nC\nLW\nRW\nC\nLW\nRW\n\n\nAuston Matthews\n73\n0\n0\n27.57\n2.47\n73\n0\n0\n73\n0\n0\n\n\nJohn Tavares\n79\n0\n0\n22.8\n1.01\n79\n0\n0\n79\n0\n0\n\n\nDavid Kampf\n82\n0\n0\n19.31\n0.95\n82\n0\n0\n82\n0\n0\n\n\nJason Spezza\n61\n1\n9\n9.54\n-2.61\n70\n0\n1\n62\n3\n6\n\n\nBrett Seney\n2\n0\n0\n0.55\n1.72\n1\n1\n0\n2\n0\n0\n\n\nColin Blackwell\n11\n0\n8\n-3.1\n-1.58\n5\n0\n14\n9\n0\n10\n\n\nNicholas Abruzzese\n1\n5\n3\n-4.95\n0.03\n1\n7\n1\n1\n6\n2\n\n\nKirill Semyonov\n1\n1\n1\n-5.77\n0.28\n0\n3\n0\n1\n1\n1\n\n\nAlexander Kerfoot\n12\n53\n17\n-6.24\n0.7\n10\n72\n0\n12\n54\n16\n\n\nAlex Steeves\n0\n0\n3\n-6.31\n-0.27\n2\n0\n1\n0\n0\n3\n\n\nMichael Amadio\n0\n1\n2\n-6.35\n-0.99\n0\n0\n3\n0\n3\n0\n\n\nKyle Clifford\n1\n15\n7\n-6.73\n0.22\n3\n15\n5\n1\n19\n3\n\n\nWayne Simmonds\n0\n31\n41\n-7.51\n-0.23\n1\n11\n60\n0\n9\n63\n\n\nPierre Engvall\n4\n43\n31\n-7.94\n0.17\n0\n59\n19\n4\n44\n30\n\n\nNicholas Robertson\n0\n7\n3\n-7.99\n-0.04\n0\n8\n2\n0\n9\n1\n\n\nJoey Anderson\n0\n1\n4\n-8.13\n-0.19\n0\n0\n5\n0\n0\n5\n\n\nNick Ritchie\n0\n25\n8\n-8.94\n0.48\n0\n33\n0\n0\n33\n0\n\n\nOndrej Kase\n0\n11\n39\n-9.21\n-0.41\n0\n3\n47\n0\n2\n48\n\n\nWilliam Nylander\n1\n26\n54\n-9.34\n-0.54\n0\n0\n81\n1\n16\n64\n\n\nIlya Mikheyev\n0\n35\n18\n-9.82\n0.25\n0\n38\n15\n0\n52\n1\n\n\nMichael Bunting\n0\n51\n28\n-10.72\n0.21\n0\n77\n2\n0\n76\n3\n\n\nMitchell Marner\n0\n21\n51\n-11.3\n-0.18\n0\n1\n71\n0\n0\n72\n\n\n\nAs for the entire league, 36229 out of 47208 come to a concensus for all three options. The remaining conflicts are resolved in the following manner. First by majority vote, then by sorting out the remaining labels for each game and team with the single game strength terms."
  },
  {
    "objectID": "posts-hockey/player-positions/index.html#concluding-remarks",
    "href": "posts-hockey/player-positions/index.html#concluding-remarks",
    "title": "Correcting Player Positions",
    "section": "Concluding Remarks",
    "text": "Concluding Remarks\nThere remains a lot to be tinkered with. Adding penalties, separating wrap-arounds or other secondary types from shots, partitioning the rink’s grid by zone before smoothing, possibly some more denoising… I suspect most to be fruitless. The main sticking point is distilling winger labels. Diminishing the additive smoothing or pushing the KDE to produce more discriminative ratios leads to similar yet murky results. Forwards tend to cross over the \\(y = 0\\) line enough to require more spatial sampling for consistency. The only way I’ve found to overcome this is by feeding season data into the single game strength terms, in perhaps the most unprincipled manner (please feel free to lecture me on my bad behaviour). I subsist this provides close to ideal results without manual inspection. It seems adequate if your goal is to get positional eligibility status for fantasy hockey. Keep scrolling down for results.\nYou can reach me on twitter at yimmymcbill if you have suggestions, there is certainly room for improvement!\nI’ve provided a summary for each team in the links below.\nAnaheim Ducks Arizona Coyotes Boston Bruins Buffalo Sabres Calgary Flames Carolina Hurricanes Chicago Blackhawks Colorado Avalanche Columbus Blue Jackets Dallas Stars Detroit Red Wings Edmonton Oilers Florida Panthers Los Angeles Kings Minnesota Wild Montréal Canadiens Nashville Predators New Jersey Devils New York Islanders New York Rangers Ottawa Senators Philadelphia Flyers Pittsburgh Penguins San Jose Sharks Seattle Kraken St. Louis Blues Tampa Bay Lightning Toronto Maple Leafs Vancouver Canucks Vegas Golden Knights Washington Capitals Winnipeg Jets"
  },
  {
    "objectID": "posts-hockey/player-positions/index.html#references",
    "href": "posts-hockey/player-positions/index.html#references",
    "title": "Correcting Player Positions",
    "section": "References",
    "text": "References\n[1] Firth, D., Kosmidis, I., & Turner, H. (2019). Davidson-Luce model for multi-item choice with ties. arXiv preprint arXiv:1909.07123.\n\nDefender Results\n The final estimates for the coefficient-like terms for the event counts in the posterior odds ratio formula. The log of the LD over RD rate parameters.\n The final Rates per sixty minutes for left and right defenders\n\n\nForward Results\n The final estimates for the coefficient-like terms for the event counts in the posterior odds ratio formula between two of the three forward positions.\n An example using Gaussian KDE with \\(\\sigma\\) cranking up to 20.\n The final Rates per sixty minutes for all forward positions"
  },
  {
    "objectID": "posts-hockey/player-positions/index.html#forwards-labelled-as-defenders",
    "href": "posts-hockey/player-positions/index.html#forwards-labelled-as-defenders",
    "title": "Correcting Player Positions",
    "section": "Forwards Labelled as Defenders",
    "text": "Forwards Labelled as Defenders\n\n\n\n\n\n\n\n\nplayerId\nfullName\ngameId\n\n\n\n\n8474145\nRobert Bortuzzo\n2021020311\n\n\n8474722\nLuke Witkowski\n2021020939\n\n\n8475625\nMatt Irwin\n2021020469\n\n\n8476372\nNick Seeler\n2021020739\n\n\n8476470\nNathan Beaulieu\n2021020314\n\n\n8476779\nBrad Hunt\n2021021152\n\n\n8477073\nKurtis MacDermid\n2021020299, 2021020321, 2021020399, 2021020415, 2021020433, 2021020490, 2021020580, 2021020591, 2021020606, 2021020622, 2021020634, 2021020641, 2021020663, 2021020805, 2021020826, 2021020836, 2021020857, 2021020871, 2021020886, 2021020962, 2021020982, 2021021119\n\n\n8477335\nKyle Burroughs\n2021021175, 2021021192\n\n\n8477419\nMason Geertsen\n2021020043, 2021020056, 2021020090, 2021020226, 2021020327, 2021020360, 2021020422, 2021020529, 2021020586, 2021020631, 2021020666, 2021020817, 2021020875, 2021020904, 2021020921, 2021020935, 2021020966, 2021020989, 2021021005, 2021021074, 2021021274\n\n\n8477851\nJordan Oesterle\n2021020472, 2021021101, 2021021222, 2021021235\n\n\n8477938\nHaydn Fleury\n2021020967\n\n\n8478013\nJake Walman\n2021020394, 2021020547\n\n\n8478017\nMark Friedman\n2021021029\n\n\n8479372\nJosh Mahura\n2021020907\n\n\n8479376\nVictor Mete\n2021021199\n\n\n8479439\nJacob MacDonald\n2021020105, 2021020122, 2021020145\n\n\n8479639\nDylan Coghlan\n2021021073, 2021021126\n\n\n8480160\nRadim Simek\n2021021171\n\n\n8480884\nCalen Addison\n2021020344, 2021020599\n\n\n8481003\nHunter Drew\n2021021285, 2021021307\n\n\n8482624\nDaniil Miromanov\n2021020100"
  }
]